{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The STEPS FOR DATA ANALISYS AND ML PROBLEMS :\n",
    "- 1 - load and plot (optional) data to understand the problem\n",
    "- 2 - (optional, recommended) data normalization / standardization - great influence on models that estimate on distances - gradient descent much faster\n",
    "- 3 - (optional) feature selection / dimensionality reduction\n",
    "- 4 - algorithm comparison and choice - train with cross validation ----- NB : scoring!!\n",
    "- 5 - validation curve / learning curve\n",
    "- 6 - (optional) algorithm tuning\n",
    "- 7 - make predictions - confusion matrix & classification report\n",
    "\n",
    "\n",
    "NB : always feature selection AFTER feature scaling\n",
    "\n",
    "ps -> no validation curve or learning curve in this file. Search in the other if needed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:00:11.892516Z",
     "start_time": "2025-12-14T17:00:10.111634Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, RFE\n",
    "\n",
    "#classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#regression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge #least squares\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:00:12.155785Z",
     "start_time": "2025-12-14T17:00:11.902060Z"
    }
   },
   "source": [
    "data = pd.read_csv('weather.csv', sep=';')"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'weather.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m data = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mweather.csv\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m;\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Big Data/BData/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Big Data/BData/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Big Data/BData/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Big Data/BData/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Uni/Big Data/BData/lib/python3.13/site-packages/pandas/io/common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'weather.csv'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for nan values\n",
    "data[data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for all zeros columns\n",
    "data.columns[(data == 0).all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset bilanciato 50:50\n",
    "data.groupby('y').size().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize one column distribution\n",
    "data.groupby('age').size().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting rows for each unique value (feature)\n",
    "data.groupby('Location').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting with multi indexing - groupby su più chiavi\n",
    "\n",
    "toplot = data.groupby(['marital', 'y']).size().unstack()\n",
    "toplot.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBB\n",
    "# plotting tha same things as before, but a graph for each first key\n",
    "toplot = data.groupby(['marital', 'y']).size()\n",
    "\n",
    "for i in toplot.index.levels[0]:\n",
    "    toplot.loc[i].plot(kind='bar', title=i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per capire se una classe influisce su quella di predizione ---- y è quella di predizione\n",
    "toplot = dataset.groupby(['marital', 'y']).size()\n",
    "total = dataset.groupby('y').size()\n",
    "# automaticamente posso dividere per ogni indice\n",
    "(toplot / total).unstack().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plottare la media di una coppia di feature\n",
    "toplot = dataset.groupby(['sex','G3']).size()# do .unstack() after, so we can divide by index\n",
    "total = dataset.groupby('sex').size()\n",
    "\n",
    "(toplot / total).unstack().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot multiple curve on same graph\n",
    "padri.plot(label='padri')\n",
    "madri.plot(label='madri')\n",
    "plt.xlabel('education')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per identificare una picco in un istogramma su una feature\n",
    "data['satisfaction_level'].hist(bins=100) # guardo istogramma per vedere numero di istanze\n",
    "data.groupby('satisfaction_level').size() # associo valore ad istanze\n",
    "data[data['satisfaction_level'] <= 0.11] # seleziono istanze\n",
    "# confronto con dati normali con describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# media e mediana di una feature\n",
    "data['satisfaction_level'].describe()\n",
    "\n",
    "# usare describe per confrontare due dataset / colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hum = data[['Month', 'Location', 'Humidity9am', 'Humidity3pm']]\n",
    "# per ogni coppia citta mese calcolo le temperature minime e massime calcolate al mattino e al pome\n",
    "hum_max = hum.groupby(['Location', 'Month']).max()\n",
    "hum_min = hum.groupby(['Location', 'Month']).min()\n",
    "# qui cerco la massima/minima tra le due ----> mi fornisce quella giornaliera\n",
    "hum_max = hum_max.max(axis=1)\n",
    "hum_min = hum_min.min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to create a copy of a dataset\n",
    "datanew = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with nan instances\n",
    "data = data.dropna() # axis=1 drop columns\n",
    "# fillna valuese\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "data = data.drop([coltoremove], axis=1)\n",
    "# drop rows\n",
    "dataset_n = dataset_n.drop(toremove.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#come si aggiungono colonne in modo idiomatico --- NB - viene messa alla fine\n",
    "\n",
    "# new attributo escursione termica\n",
    "data['escursione'] = data['MaxTemp'] - data['MinTemp']\n",
    "\n",
    "# in alternativa va bene anche \n",
    "tmp = pd.DataFrame(data['MaxTemp'] - data['MinTemp'], columns=['escursione'])\n",
    "\n",
    "#metto la colonna all'inizio -- si potrebbe anche concatenare (data[:, -1:],data[:, :-1])\n",
    "data = tmp.join(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimpiazzare valori nominali con numeri\n",
    "data = data.replace(['Yes', 'No'],[1,0])\n",
    "# same ----> data.replace({'yes':1, 'no':0})\n",
    "\n",
    "#rimpiazzo solo una colonna\n",
    "cities = data['Location'].unique()\n",
    "data['Location'] = data['Location'].replace(cities, np.arange(len(cities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scegliere solo un subset di colonne da rimpiazzare\n",
    "data.columns[data.dtypes == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimpiazzo tutte le colonne con valori numerici\n",
    "for col in tomap.columns:\n",
    "    uniquev = tomap[col].unique()\n",
    "    tomap.loc[:, col] = tomap.loc[:, col].replace(uniquev, np.arange(len(uniquev))) # loc non è necessario ma non da warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always check values after\n",
    "data.dtypes\n",
    "# if some values still not int\n",
    "data['Location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies features su TUTTO il dataset ------ NB : ignora i valori float!!!!!! BUONO!\n",
    "# NON IMPORTA SE I VALORI SONO STRINGHE ------ NON SERVE CONVERTIRE LE FEATURE IN NUMERI\n",
    "data_dummy = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creare un nuovo dataframe con dummies features solo su determinate colonne\n",
    "\n",
    "new_data = data_numeric.copy()\n",
    "\n",
    "todummy = 'age_new' # colonne da dummizzare could be data.dtypes == 'object'\n",
    "dummies = pd.get_dummies(data_numeric[todummy])\n",
    "\n",
    "new_data = new_data.drop(todummy, axis=1)\n",
    "new_data = new_data.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creare nuovi dataset\n",
    "# new datasets from orginal dataset\n",
    "cloudP = datini[datini['Cloud3pm'] < 0]\n",
    "cloudT = datini[-(datini['Cloud3pm'] < 0)] # the sign - revert a boolean series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ricevere dati numeri da dataframe\n",
    "data_numeric = data._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database basato solo su alcune colonne\n",
    "data_color = data.loc[:, [0,3,9,14,15,17,20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eventuale renaming delle colonne\n",
    "data_color = data_color.rename(columns={0:'p',\n",
    "                                        3:'c1',\n",
    "                                        9:'c2',\n",
    "                                        14:'c3',\n",
    "                                        15:'c4',\n",
    "                                        17:'c5',\n",
    "                                        20:'c6'})\n",
    "\n",
    "# se voglio rinominare tutte le colonne in con una lista\n",
    "data_color.columns = ['p', 'c1', 'c2',...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET BALANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance a data with resampling\n",
    "# bigger rules or intermidiate value\n",
    "from sklearn.utils import resample\n",
    "\n",
    "bigger = data_air[data_air['SIZE']==1] \n",
    "smaller = data_air[data_air['SIZE']==3]\n",
    "\n",
    "# bilanciamo le classi \n",
    "smaller = resample(smaller, replace=True, n_samples=5000) # can be also n_samples=len(bigger)\n",
    "bigger = resample(bigger, replace=True, n_samples=5000) # non cessary if previous n_samples = len(bigger)\n",
    "\n",
    "data_bal = pd.concat([bigger, smaller])\n",
    "data_bal.groupby('SIZE').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB posso spezzare i dati in mquesto modo se non mi è stata fornita nessuna specifica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nel caso la colonna di classificazione non sia in fondo possiamo usare questa notazione\n",
    "X = data.drop('Cloud3pm', axis=1)\n",
    "y = data['Cloud3pm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividere il dataset in un N intervalli \n",
    "# ---> ritorna array contenente indice di appartenenza ad un bin\n",
    "interval = pd.cut(data_numeric['age'], bins=3, labels=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nel caso dobbiamo dividere per ogni classe di size\n",
    "# NBBBB-----> automaticamente con train_test_split() prendendo i valori a random\n",
    "# ----------> le proprietà dovrebbero rimane quasi rispettate (ma non sarà perfetto)\n",
    "\n",
    "index1 = data_bal['SIZE'] == 1\n",
    "index3 = data_bal['SIZE'] == 3\n",
    "index5 = data_bal['SIZE'] == 5\n",
    "\n",
    "X_1 = data_bal[index1].drop('SIZE', axis=1)\n",
    "y_1 = data_bal.loc[index1, 'SIZE']\n",
    "\n",
    "X_3 = data_bal[index3].drop('SIZE', axis=1)\n",
    "y_3 = data_bal.loc[index3, 'SIZE']\n",
    "\n",
    "X_5 = data_bal[index5].drop('SIZE', axis=1)\n",
    "y_5 = data_bal.loc[index5, 'SIZE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# andiamo poi ad unire le classi\n",
    "fraction = 0.25\n",
    "\n",
    "X_1train, X_1test, y_1train, y_1test = train_test_split(X_1, y_1, test_size=fraction)\n",
    "X_3train, X_3test, y_3train, y_3test = train_test_split(X_3, y_3, test_size=fraction)\n",
    "X_5train, X_5test, y_5train, y_5test = train_test_split(X_5, y_5, test_size=fraction)\n",
    "\n",
    "X_train = pd.concat([X_5train, X_3train, X_1train])\n",
    "X_test = pd.concat([X_5test, X_3test, X_1test])\n",
    "\n",
    "y_train = pd.concat([y_5train, y_3train, y_1train])\n",
    "y_test = pd.concat([y_5test, y_3test, y_1test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y to ndarray -----> solo se fatto senza train_test_split()\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y = np.ravel(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non vado a normalizzare le componenti che sono attributi di tipo nominale, NON AVREBBE SENSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer()\n",
    "#X_new_train = norm.fit_transform(X_train)\n",
    "#X_new_test = norm.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ora procedo al dimensionality reduction con pca\n",
    "pca = PCA(n_components=10)\n",
    "#X_new_train = pca.fit_transform(X_new_train)\n",
    "#X_new_test = pca.fit_transform(X_new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oppure con univariate\n",
    "uni_sel = SelectKBest(k=10)\n",
    "#X_new = uni_sel.fit_transform(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oppure con recursive feature elimination\n",
    "#rfe = RFE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribuzione delle feature\n",
    "tmp = pd.DataFrame(X_train)\n",
    "tmp.hist(figsize=(12,5), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter matrix\n",
    "# può essere molto lento, da evitare con grandi dataset e molte features\n",
    "# pd.plotting.scatter_matrix(tmp)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "plt.matshow(tmp.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement a model (eg. logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testare accuracy su modello\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000) # parameter change for each algorithms look down\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "print('accuracy :')\n",
    "print(accuracy_score(y_test, predicted))\n",
    "print('confusion matrix :')\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print('classification report :')\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running cross validation test on algoritmhs - accuracy/f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation on single model\n",
    "model = DecisionTreeClassifier()\n",
    "num_folds = 5\n",
    "scoring = 'accuracy'\n",
    "\n",
    "results = cross_val_score(model, X, y, scoring=scoring, cv=num_folds)\n",
    "\n",
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm choice\n",
    "\n",
    "models = []\n",
    "\n",
    "\n",
    "#CLASSIFICATION\n",
    "\n",
    "#models.append(('lSVM', LinearSVC() ))\n",
    "#models.append(('SVM', SVC(gamma='scale') )) # can be also 'auto'\n",
    "#models.append(('p2SVM', SVC(gamma='scale', kernel='poly', degree=2) ))\n",
    "#models.append(('p3SVM', SVC(gamma='scale', kernel='poly', degree=3) ))\n",
    "#models.append(('p4SVM', SVC(gamma='scale', kernel='poly', degree=4) ))\n",
    "#models.append(('AB', AdaBoostClassifier() ))\n",
    "#models.append(('NB', GaussianNB() ))\n",
    "#models.append(('MNB', MultinomialNB() ))\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', max_iter=1500) ))\n",
    "models.append(('DT', DecisionTreeClassifier() ))\n",
    "#models.append(('RF', RandomForestClassifier(n_estimators=10) ))\n",
    "#models.append(('NN', MLPClassifier(max_iter=5000) )) troppo lento\n",
    "#models.append(('KNN', KNeighborsClassifier() ))\n",
    "\n",
    "# REGRESSION\n",
    "#models.append(('lSVR', LinearSVR() ))\n",
    "#models.append(('SVR', SVR(gamma='scale') ))\n",
    "#models.append(('p2SVM', SVR(gamma='scale', kernel='poly', degree=2) ))\n",
    "#models.append(('p3SVM', SVR(gamma='scale', kernel='poly', degree=3) ))\n",
    "#models.append(('p4SVM', SVR(gamma='scale', kernel='poly', degree=4) ))\n",
    "#models.append(('AB', AdaBoostRegressor() ))\n",
    "#models.append(('LR', LinearRegression() ))\n",
    "#models.append(('DT', DecisionTreeRegressor() ))\n",
    "#models.append(('RI', Ridge() ))\n",
    "#models.append(('LA', Lasso() ))\n",
    "#models.append(('RF', RandomForestRegressor(n_estimators=10) ))\n",
    "#models.append(('KNN', KNeighborsRegressor() ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# personalized scorer - eg. measuring accuracy of rounded regression results WITH CROSS VALIDATION\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def accuracy_reg(y, y_pred, **kwargs):\n",
    "    y_pred = np.round(y_pred)\n",
    "    errors = np.sum(np.abs(y - y_pred))\n",
    "    correct = len(y) - errors\n",
    "    return correct/len(y)\n",
    "\n",
    "# personal scorer instance\n",
    "accuracy_r = make_scorer(accuracy_reg)\n",
    "scoring = accuracy_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring\n",
    "# classification\n",
    "scoring = 'accuracy' # 'f1_micro'\n",
    "# regression\n",
    "#scoring ='neg_mean_squared_error' # 'neg_mean_absolute_error' \n",
    "\n",
    "# testing models\n",
    "results = []\n",
    "names = []\n",
    "num_folds = 5\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    t = time.process_time()\n",
    "    # be careful, use (X, y) or (X_train, y_train) depending on how data were splitted\n",
    "    cv_result = cross_val_score(model, X_train, y_train, scoring=scoring, cv=num_folds)\n",
    "    t = time.process_time() - t\n",
    "    results.append(cv_result)\n",
    "    names.append(name)\n",
    "    \n",
    "    print('time required by', name,' : ', t)\n",
    "    \n",
    "    #the confusione matrix è la matrice dei true positive ecc...\n",
    "    \n",
    "    #Xtrain, Xtest, ytrain, ytest = train_test_split(X_new, y)\n",
    "    #model.fit(Xtrain, ytrain)\n",
    "    #predicted = model.predict(Xtest)\n",
    "    #print(confusion_matrix(ytest, predicted))\n",
    "print('results for ', scoring, ':')\n",
    "frame = pd.DataFrame(results, index=names)\n",
    "frame.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm comparison accuracy')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(frame)\n",
    "ax.set_xticklabels(frame.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
